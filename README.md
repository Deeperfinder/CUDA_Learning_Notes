# CUDA_Learning_Notes
包含GEMM， FLash Attention的cute、cuda实现
## GEMM
- 目前在SM80上实现了Cute的简单GEMM  + 流水线GEMM（multi-stage）
* 通过python biding和torch load 来构建算子的benchmark

CUDA test 1 (naive / sliced k)
```bash
------------------------------------GEMM lib build from sources-------------------------------------
----------------Loading hgemm lib on device: NVIDIA H20, capability: (9, 0), : None-----------------
----------------------------------------------------------------------------------------------------
---------------------------------------M=1024, N=1024, K=512----------------------------------------
                      naive_fp16: ['-7.2929687', '0.0       '], time:0.0001825820690598981ms, TFLOPS:5.88  (+0.00%)
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
---------------------------------------M=2048, N=2048, K=1024---------------------------------------
                      naive_fp16: ['-32.53125 ', '0.0       '], time:0.0013114626160983381ms, TFLOPS:6.55  (+0.00%)
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
---------------------------------------M=4096, N=4096, K=4096---------------------------------------
                      naive_fp16: ['-64.1875  ', '-23.265625'], time:0.0426098966927364ms, TFLOPS:3.23  (+0.00%)
                   sliced_k_fp16: ['-64.1875  ', '-23.265625'], time:0.021670748743517643ms, TFLOPS:6.34  (+96.62%)
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
---------------------------------------M=8192, N=8192, K=8192---------------------------------------
                      naive_fp16: ['30.71875  ', '46.90625  '], time:0.42255887998383623ms, TFLOPS:2.60  (+0.00%)
                   sliced_k_fp16: ['30.71875  ', '46.90625  '], time:0.18463349598851703ms, TFLOPS:5.96  (+128.86%)
```
